<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- 文件输出格式 -->
    <property name="PATTERN" value="%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] [%X{trace_id}] %-5level %logger{36} - %msg%n"/>
    <!-- pro文件路径 -->
    <property name="log.path" value="/home/log/java/foodie"/>
    <!-- 读取application.yml配置 -->
    <springProperty scope="context" name="environment" source="spring.profiles.active" />
    <springProperty scope="context" name="bootstrap.servers" source="logback.kafka.bootstrap.servers" />
    
    <!-- 输出到控制台-->
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息-->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>DEBUG</level>
        </filter>
        <encoder>
            <Pattern>${PATTERN}</Pattern>
            <!-- 设置字符集 -->
            <charset>UTF-8</charset>
        </encoder>
    </appender>
    
    <!-- ch.qos.logback.core.rolling.RollingFileAppender 文件日志输出 -->
    <appender name="PROD_FILE"  class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/shs.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] [%X{trace_id}] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/shs-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>3</maxHistory>
        </rollingPolicy>
     </appender>
     
    <!-- 如果kafka无法连接统一输出到一个日志文件 -->
    <appender name="NO_KAFKA_FILE"  class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!-- 正在记录的日志文档的路径及文档名 -->
        <file>${log.path}/shs_no_kafka.log</file>
        <!--日志文档输出格式-->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] [%X{trace_id}] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- 此处设置字符集 -->
        </encoder>
        <!-- 日志记录器的滚动策略，按日期，按大小记录 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>${log.path}/shs-no-kafka-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
            <!--日志文档保留天数-->
            <maxHistory>3</maxHistory>
        </rollingPolicy>
     </appender>
     
    <!-- 日志发送到kafka -->
    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <encoder charset="UTF-8" class="net.logstash.logback.encoder.LogstashEncoder" >
            <customFields>{"appName":"foodie{environment}","HOSTNAME":"${HOSTNAME}"}</customFields>
            <includeMdc>true</includeMdc>
            <includeMdcKeyName>trace_id</includeMdcKeyName>
            <includeContext>false</includeContext>
            <fieldNames>
                <!--  <timestamp>timestamp</timestamp> -->
                <version>[ignore]</version>
                <levelValue>[ignore]</levelValue>
            </fieldNames>
            <throwableConverter class="net.logstash.logback.stacktrace.ShortenedThrowableConverter">
                <maxDepthPerThrowable>30</maxDepthPerThrowable>
                <rootCauseFirst>true</rootCauseFirst>
            </throwableConverter>
        </encoder>

        <topic>test</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.NoKeyKeyingStrategy" />
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />
        <producerConfig>bootstrap.servers=123.57.69.155:9092</producerConfig>
        <producerConfig>compression.type=gzip</producerConfig>
        <producerConfig>retries=1</producerConfig>
        <producerConfig>acks=0</producerConfig>
        <producerConfig>request.timeout.ms=30000</producerConfig>
        <!-- wait up to 1000ms and collect log messages before sending them as a batch -->
        <producerConfig>linger.ms=1000</producerConfig>
        <!-- even if the producer buffer runs full, do not block the application but start to drop messages -->
        <producerConfig>max.block.ms=30000</producerConfig>
        <!-- this is the fallback appender if kafka is not available. -->
        <appender-ref ref="NO_KAFKA_FILE" />
     </appender>
    
     <!-- local本地开发环境 -->
    <springProfile name="local">
        <root level="INFO">
            <appender-ref ref="CONSOLE"/>
            <appender-ref ref="kafkaAppender"/>
<!--            <appender-ref ref="PROD_FILE"/>-->
        </root>
    </springProfile>

    <!-- dev环境 -->
    <springProfile name="dev">
        <root level="INFO">
            <appender-ref ref="kafkaAppender"/>
        </root>
    </springProfile>

    <!-- 测试环境 -->
    <springProfile name="test">
        <root level="INFO">
            <appender-ref ref="kafkaAppender"/>
        </root>
    </springProfile>
    
    <!-- demo环境 -->
    <springProfile name="demo">
        <root level="INFO">
            <appender-ref ref="PROD_FILE"/>
        </root>
    </springProfile>

    <!-- 生产环境 -->
    <springProfile name="prod">
        <root level="INFO">
            <appender-ref ref="kafkaAppender"/>
        </root>
    </springProfile>
</configuration>